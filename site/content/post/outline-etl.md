---
title: 离线ETL和实时ETL
date: 2019-12-16 21:44:17
description: 如何有效的实现ETL功能.
---

*前言*

### 大数据虽然是时下热门的话题.很多人想学习.但是网上存着大量低质量的内容.
本文希望通过简单有效的一个实例把大数据工程讲清楚.为后人少走弯路做点贡献.

1. 为什么要使用大数据技术链.
2. 最后要实现什么.
3. 怎么实现.

先来一个最简单的例子.


最后来一个实际使用中工程化的案例.

#### 一个完整的大数据工程包含了以下模块.

1. 数据采集
2. 数据清洗
3. 数据挖掘
4. 内容生成
5. SEO
6. 驱动用户加入
7. 相关数据关系归纳和汇总.
8. 找到增长点

数据存储格式应该保持一致性.
并且在入库同时就要排除掉一部分数据.
比如,重复的,空的,无效的.
最好从最终功能需求中推测出需要的特征值.对这些值进行统一命名.

格式:由于现在抓取来的数据一般格式都比较复杂.所以.倒是可以统一成json格式,存储在Mongodb或者Hbase中.方便以后批量解析.或者做转换.
数据仓库的概念中,一个是有数据入仓时间.还有一个是原始数据不再进行修改.到另一层再进行处理.这样就可以保证增量数据可以通过一个标示来不进行全量计算.

字段:一开始的时候每个源的字段肯定不同,尽量预测后期可能使用的字段,以及多个源类似的字段.一致命名.

数据关联(外键):如果是多源数据.就涉及数据之间关系使用那个字段.
比如电话号码.或者名字.这需要一个完整的规则.

优先级:如果是多源数据合并.相同重复的字段.就会有一个优先级概念.使用那个源头做优先级.或者用某种策略比如哪里的数值大.来做优先级.这个必须明确.

当各个数据源头的数据分别入库后.

使用Spark进行一次初步清洗.统一不一致的相同字段的格式. 比如日期.分值.电话格式.地址格式等.
数据进入拼接前准备.

将多个数据表进行Join,或者一个包含所有需要特征的大表.并将结果存储到Hive中待用.
这个汇总大表的单条数据是可以更新的.如果数据源中增加了新的数据或者更新了一些字段.应该可以不全量处理,而是拿出来单条进行处理.

通过大表进行运算.运算结果可以追加到记录后面,以备提取和方便验证.

对于当前应用来说,通过各个特征进行运算就能得到最终需要的数据.其实是对单条数据的处理.


抽取出需要的数据,更新或者插入到业务库中.或者输出到汇总结果集中.

把这个过程增量处理好,就是流运算了.

对于一个实体的多个特征之间进行运算可以得到相似度.

使用分布式技术来提高运算速度.方便进行伸缩.以及存储和迁移.

单机
容量有限
算力有限

分布式
容量可无限扩充
算力可扩展.

批处理
需要对全集进行计算.比如需要算出某个实体在整个特征集中的顺序.或者全量数据的汇总,累加.

流运算.
对于某个实体在某一个时间范围内的汇总.

Mapreduce.
映射 汇总.


